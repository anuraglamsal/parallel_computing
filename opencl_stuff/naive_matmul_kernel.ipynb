{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a naive OpenCL kernel for matmul and comparing that kernel with numpy's CPU implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl \n",
    "import numpy as np\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()\n",
    "# to time more precisely/fairly, enabling profile.\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl = \"\"\"\n",
    "__kernel void mat_mul_multi_core(__global float *res, __global float *first, __global float *second){\n",
    "    int idx_row = get_global_id(0);\n",
    "    int idx_col = get_global_id(1);\n",
    "    int size = get_global_size(0);\n",
    "    // using memory closest to the core is probably better than using global memory. have to think \n",
    "    // about this more though. \n",
    "    float val = 0; \n",
    "    for(int i=0; i<size; ++i){\n",
    "       //btw, memory is contiguously allocated, so indexing like this. Stride = (size, 1).\n",
    "        val += first[size*idx_row+i] * second[size*i+idx_col];\n",
    "    }\n",
    "    res[size*idx_row+idx_col] = val;\n",
    "}\n",
    "\"\"\"\n",
    "krnl_prog = cl.Program(ctx, krnl).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = cl.mem_flags\n",
    "\n",
    "sz = 4096 # the size of the matrix is sz x sz. you can play with this. \n",
    "\n",
    "first_host = np.random.uniform(0, 1, size = (sz, sz)).astype(np.float32)\n",
    "first_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=first_host)\n",
    "\n",
    "second_host = np.random.uniform(0, 1, size = (sz, sz)).astype(np.float32)\n",
    "second_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=second_host)\n",
    "\n",
    "result = cl.Buffer(ctx, mf.WRITE_ONLY, first_host.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://man.opencl.org/clGetEventProfilingInfo.html : OpenCL profiling options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU speeds measured in different ways = (5.60436337502324, 5.56757392, 5.604012)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krnl_second_obj = krnl_prog.mat_mul_multi_core\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "event = krnl_second_obj(queue, (sz, sz), (16, 16), result, first_device, second_device)\n",
    "event.wait()\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "\n",
    "\"\"\"\n",
    "# if you want to see the result of the computation.\n",
    "res_np = np.empty_like(first_host)\n",
    "cl.enqueue_copy(queue, res_np, result)\n",
    "print(res_np)\n",
    "\"\"\"\n",
    "\n",
    "# in ths measurement, there is probably extra overhead because of pyopencl. i mean it is a wrapper on \n",
    "# top of the actual OpenCL API written in C right? but even with this, gpu_speed and gpu_speed_3  \n",
    "# tend to be pretty close. in actual practice, you'll probably write purley in C, or using a low level \n",
    "# library like tinygrad's gpuctypes for instance. so, i think using gpu_speed_3 is going\n",
    "# to be more accurate. gpu_speed probably also measured the time taken to enqueue the command, \n",
    "# but that time is probably insignificant anyways. \n",
    "gpu_speed = (\n",
    "    end_time - start_time\n",
    ") \n",
    "\n",
    "# difference between start of execution of kernels and end of execution of kernels.\n",
    "# the time is in nanosecond, thus the * 1e-9. \n",
    "gpu_speed_2 = (\n",
    "    event.profile.end - event.profile.start\n",
    ") * 1e-9 \n",
    "\n",
    "# difference between the time when the command is enqueued till the command\n",
    "# is over (including all the child commands corresponding to the given command.)\n",
    "gpu_speed_3 = (\n",
    "    event.profile.complete - event.profile.queued\n",
    ") * 1e-9 \n",
    " \n",
    "# for larger matrices, all of these are more or less the same; the difference isn't significant. \n",
    "f\"GPU speeds measured in different ways = {gpu_speed, gpu_speed_2, gpu_speed_3}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CPU speed = 1.0374637419881765'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "res = first_host @ second_host # numpy's matmul. This occurs on the CPU. \n",
    "\n",
    "end_time = time.perf_counter()\n",
    "    \n",
    "CPU_speed =  end_time - start_time\n",
    "\n",
    "# np.show_config()\n",
    "\n",
    "f\"CPU speed = {CPU_speed}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU is roughly 5x as fast as the GPU.\n"
     ]
    }
   ],
   "source": [
    "if CPU_speed < gpu_speed_3:\n",
    "    speedup = round(gpu_speed_3 / CPU_speed)\n",
    "    if speedup == 1:\n",
    "        print(f\"They are roughly the same speed. CPU is a bit faster.\")\n",
    "    else:\n",
    "        print(f\"CPU is roughly {speedup}x as fast as the GPU.\")\n",
    "else:\n",
    "    speedup = round(CPU_speed / gpu_speed_3)\n",
    "    if speedup == 1:\n",
    "        print(f\"They are roughly the same speed. GPU is a bit faster.\")\n",
    "    else:\n",
    "        print(f\"GPU is roughly {speedup}x fast as the CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with different sizes of the matrices, the range of numbers in the matrices, workgroup sizes, etc. In general, what you find is that numpy's CPU implementation outperforms the GPU irrespective of what you do, for this kernel at least. Sometimes the CPU can be inconsistent, probably because there is a bunch of stuff running on the CPU while the GPU is mostly idle, but nothing significant. In some configurations, their speeds are roughly the same or GPU wins by a very insignificant margin, while in most configurations (and practical scenarios), the CPU is significantly better. Numpy's is just better. Maybe a different kernel would do better. Will try. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
