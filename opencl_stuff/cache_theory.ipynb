{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out the kernel I wrote in \"naive_matmul_kernel.ipynb\" isn't correct. It's not technically incorrect, <br>\n",
    "but it's incorrect in terms of what I intended to do. I wanted to do row-major matmul, but I ended up doing column-major matmul. <br><br>\n",
    "https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Row_and_column_major_order.svg/340px-Row_and_column_major_order.svg.png<br><br>\n",
    "So, I corrected it without expecting anything, but it ended up giving me great performance improvement. This notebook essentially <br>\n",
    "explores the \"why\" behind the performance improvement. <br>\n",
    "\n",
    "The \"why\" is explored at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "num_of_test_loops = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Col major kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_col_major = \"\"\"\n",
    "__kernel void mat_mul_col(__global float *res, __global float *first, __global float *second){\n",
    "    int idx_row = get_global_id(0);\n",
    "    int idx_col = get_global_id(1);\n",
    "    int size = get_global_size(0);\n",
    "    float val = 0; \n",
    "    for(int i=0; i<size; ++i){\n",
    "       //btw, memory is contiguously allocated, so indexing like this. Stride = (size, 1).\n",
    "        val += first[size*idx_row+i] * second[size*i+idx_col];\n",
    "    }\n",
    "    res[size*idx_row+idx_col] = val;\n",
    "}\n",
    "\"\"\"\n",
    "krnl_prog_col = cl.Program(ctx, krnl_col_major).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row major kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_row_major = \"\"\"\n",
    "__kernel void mat_mul_row(__global float *res, __global float *first, __global float *second){\n",
    "    // just flip 1 to 0 and 0 to 1 from above. \n",
    "    // This is what was intended. \n",
    "    int idx_row = get_global_id(1); \n",
    "    int idx_col = get_global_id(0);\n",
    "    \n",
    "    int size = get_global_size(0); // we're working with nxn matrix, so doesn't matter\n",
    "                                   // whether we do (0) or (1) in get_global_size. \n",
    "    float val = 0; \n",
    "    for(int i=0; i<size; ++i){\n",
    "       //btw, memory is contiguously allocated, so indexing like this. Stride = (size, 1).\n",
    "        val += first[size*idx_row+i] * second[size*i+idx_col];\n",
    "    }\n",
    "    res[size*idx_row+idx_col] = val;\n",
    "}\n",
    "\"\"\"\n",
    "krnl_prog_row = cl.Program(ctx, krnl_row_major).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stuff():\n",
    "    mf = cl.mem_flags\n",
    "\n",
    "    sz = 4096 # the size of the matrix is sz x sz. you can play with this. \n",
    "\n",
    "    first_host = np.random.uniform(0, 1, size = (sz, sz)).astype(np.float32)\n",
    "    first_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=first_host)\n",
    "\n",
    "    second_host = np.random.uniform(0, 1, size = (sz, sz)).astype(np.float32)\n",
    "    second_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=second_host)\n",
    "\n",
    "    result = cl.Buffer(ctx, mf.WRITE_ONLY, first_host.nbytes)\n",
    "    \n",
    "    return first_device, second_device, result, sz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running col major kernel. 16x16 workgroup size is better for col major matmul for some reason. If I do 8x8, it's horrendously slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_col_obj = krnl_prog_col.mat_mul_col\n",
    "\n",
    "avg_time_col = 0\n",
    "\n",
    "for i in range(num_of_test_loops):\n",
    "    first_device, second_device, result, sz = memory_stuff()\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    event = krnl_col_obj(queue, (sz, sz), (16, 16), result, first_device, second_device)\n",
    "    event.wait()\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    avg_time_col += end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running row major kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_row_obj = krnl_prog_row.mat_mul_row\n",
    "\n",
    "avg_time_row = 0\n",
    "\n",
    "for i in range(num_of_test_loops):\n",
    "    first_device, second_device, result, sz = memory_stuff()\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    event = krnl_row_obj(queue, (sz, sz), (8, 8), result, first_device, second_device)\n",
    "    event.wait()\n",
    "    \n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    avg_time_row += end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average speed for col major = 5.562838355402346 sec.\n",
      "Average speed for row major = 0.8338460477942136 sec.\n",
      "Row major matmul is 6.6713014592055835x as fast as col major matul.\n"
     ]
    }
   ],
   "source": [
    "avg_time_col /= num_of_test_loops \n",
    "avg_time_row /= num_of_test_loops \n",
    "\n",
    "speedup = avg_time_col/avg_time_row\n",
    "\n",
    "print(f\"Average speed for col major = {avg_time_col} sec.\")\n",
    "print(f\"Average speed for row major = {avg_time_row} sec.\")\n",
    "print(f\"Row major matmul is {speedup}x as fast as col major matul.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, row major is significantly faster than col major. The speed of row major is on par with numpy's CPU implementation -- even slightly better actually. Why is row major faster than col major? It is probably something to do with caching. <br>\n",
    "\n",
    "<strong>Assumption</strong>: Workgroups are mapped onto the GPU in row-major order. <br>\n",
    "\n",
    "<strong>Probability calculations</strong>: <br>\n",
    "\n",
    "<i>For \"result\" array</i>:<br>\n",
    "\n",
    "Prob. of a cell from the same row being accessed in row major matmul (Pr_R) = 1024/(1024&times;64) = 1/64 <br>\n",
    "Prob. of a cell from the same row being accessed in col major matmul (Pc_R) = 64/(1024&times;64) = 1/1024 <br>\n",
    "\n",
    "\"Pr_R\" is 16 times more likely than \"Pc_R\". <br>\n",
    "\n",
    "As elements are stored contiguously row-major, \"Pr_R\" being relatively higher than \"Pc_R\" implies higher cache hits -- chew on this. Think of locality of reference. <br>\n",
    "\n",
    "<i>For \"first\" array</i>:<br>\n",
    "\n",
    "Prob. of a cell from the same row being accessed in row major matmul (Pr_F) = (1024&times;4096)/(1024&times;4096&times;64) = 1/64 <br>\n",
    "Prob. of a cell from the same row being accessed in col major matmul (Pc_F) = (64&times;4096)/(64&times;4096&times;1024) = 1/1024 <br>\n",
    "\n",
    "\"Pr_F\" is 16 times more likely than \"Pc_F\". <br>\n",
    "\n",
    "By the same reasoning as above, here too there are higher cache hits. \n",
    "\n",
    "<i>For \"second\" array</i>:<br>\n",
    "\n",
    "Prob. of a cell from the same row being accessed in row major matmul (Pr_S) = (1024&times;64)/(1024&times;64&times;4096) = 1/4096 <br>\n",
    "Prob. of a cell from the same row being accessed in col major matmul (Pc_S) = (64&times;1024)/(64&times;1024&times;4096) = 1/4096 <br>\n",
    "\n",
    "Here, \"Pr_S\" and \"Pc_S\" are equal. So, the cache hits aren't going to be different.\n",
    "\n",
    "<strong>Overall Conclusion</strong>: Even though cache hits aren't going to be different for the \"second\" array, the fact that \"Pr_R\" is 16 times more likely than \"Pc_R\", and \"Pr_F\" is 16 times more likely than \"Pc_F\", the increased cache hits here are probably going to be significant. This, I think, explains the speedup. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
