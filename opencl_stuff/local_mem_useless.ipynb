{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the uselessness of trying to make your kernel fast through the use of local memory. <br>\n",
    "Modern GPUs have effective on-chip caches which can provide much of the benefit of Local Memory <br>\n",
    "but without programmer intervention (pg.8 of https://comp.anu.edu.au/courses/acceleratorsHPC/slides/OpenCLMemory.pdf). <br><br>\n",
    "The kernel here does matmul. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic \"naive\" matmul kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_1 = \"\"\"\n",
    "__kernel void mat_mul_1(__global float *res, __global float *first, __global float *second){\n",
    "    \n",
    "    int idx_row = get_global_id(0);\n",
    "    int idx_col = get_global_id(1);\n",
    "    int size = get_global_size(0);\n",
    "    float val = 0; \n",
    "    for(int i=0; i<size; ++i){\n",
    "        val += first[size*idx_row+i] * second[size*i+idx_col];\n",
    "    }\n",
    "    res[size*idx_row+idx_col] = val;\n",
    "}\n",
    "\"\"\"\n",
    "krnl_1_prog = cl.Program(ctx, krnl_1).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to optimize matmul via the use of local memory. The basic idea is to copy the data in local memory once, and then <br>\n",
    "do the actual computations by accessing the data from the local memory. If there was no caching, it makes sense that this<br>\n",
    "would be faster as accessing from local memory is faster than from global. But if the global memory data are cached, this <br>\n",
    "probably won't add performance benefits. \n",
    "\n",
    "About \"barrier\" and memory fence flags: https://justpaste.it/cymn1 <br>\n",
    "The code is based on \"matmul2.cl\" in this: https://public.websites.umich.edu/~smeyer/cuda/Preso07-OpenCL.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "krnl_2 = \"\"\"\n",
    "__kernel void mat_mul_2(__global float *res, __global float *first, __global float *second/*, \n",
    "__local float *first_local, __local float *second_local*/){\n",
    "    \n",
    "    int g_id_0 = get_global_id(0);\n",
    "    int g_id_1 = get_global_id(1);\n",
    "    \n",
    "    int l_id_0 = get_local_id(0);\n",
    "    int l_id_1 = get_local_id(1);\n",
    "    \n",
    "    int g_size = get_global_size(0);\n",
    "    int l_size = get_local_size(0);\n",
    "   \n",
    "   // should make these dynamic for generic kernels.\n",
    "   // but there's some problem doing that for some reason. \n",
    "    __local float first_local[8*1024];\n",
    "    __local float second_local[8*1024];\n",
    "   \n",
    "   float val = 0; \n",
    "   for(int i=0; i<g_size; i+=l_size){\n",
    "       first_local[l_id_0*g_size+l_id_1+i] = first[g_id_0*g_size+l_id_1+i];\n",
    "       second_local[(l_id_0+i)*l_size+l_id_1] = second[(l_id_0+i)*g_size+g_id_1];\n",
    "      \n",
    "       // I don't think you need CLK_GLOBAL_MEM_FENCE here. \n",
    "       // The global memory access is there to serve the local memory.\n",
    "       // The local memory write is only finished once the global memory\n",
    "       // read is done. And when the local memory write is done, the\n",
    "       // memory operations are all over. \n",
    "       \n",
    "       barrier(CLK_LOCAL_MEM_FENCE);\n",
    "       \n",
    "       for(int j=0; j<l_size; ++j){\n",
    "           val += first_local[l_id_0*g_size+j+i] * second_local[(j+i)*l_size+l_id_1];\n",
    "       }\n",
    "   }\n",
    "   res[g_id_0*g_size+g_id_1] = val;\n",
    "}\n",
    "\"\"\"\n",
    "krnl_2_prog = cl.Program(ctx, krnl_2).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function for memory allocation stuff to simulate a more or less practical scenario. <br><Br>\n",
    "If I use the same buffers for multiple runs of the kernel, then the runs after the first run are signficantly faster. <br>\n",
    "This is probably because of further cache-based optimization. It doesn't seem realistic to presume <br>\n",
    "that the same kernel with the same buffers will be executed in very short span again and again; this is <br>\n",
    "why I am allocating new buffers in every run. In reality, it is probably a mix of allocating new buffers vs <br>\n",
    "reusing buffers, but I guess I am going for the worst case scenario. Actually, an even worst-case scenario would <br>\n",
    "be to wait a bit after allocation and before execution such that the cache is completely cleared of the buffers. But <br>\n",
    "this doesn't seem realistic. One can play with these different scenarios. My experiments, till now, suggest that there are more or less <br>\n",
    "3 scenarios: <br><br>\n",
    "\n",
    "1.) When the buffers are allocated for the first time and you execute the kernel after clearing caches. - Slowest <br>\n",
    "2.) When the buffers are allocated for the first time and you execute the kernel before the caches get cleared. - Second Fastest <br>\n",
    "3.) You execute a kernel that has been executed before using the same buffers as the previous execution before caches get cleared. - Fastest <br><br>\n",
    "\n",
    "I am simulating simulating 2.) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stuff():\n",
    "    \n",
    "    mf = cl.mem_flags\n",
    "\n",
    "    sz = 1024  # the size of the matrix is sz x sz.\n",
    "\n",
    "    first_host = np.random.uniform(0, 1, size=(sz, sz)).astype(np.float32)\n",
    "    first_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=first_host)\n",
    "\n",
    "    second_host = np.random.uniform(0, 1, size=(sz, sz)).astype(np.float32)\n",
    "    second_device = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=second_host)\n",
    "\n",
    "    # first_local = cl.LocalMemory(sz*sz*4)\n",
    "    # second_local = cl.LocalMemory(sz*sz*4)\n",
    "\n",
    "    result = cl.Buffer(ctx, mf.WRITE_ONLY, first_host.nbytes)\n",
    "    \n",
    "    return sz, first_device, second_device, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the kernels multiple times and averaging the times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kernel 1 execution time: 0.0014242268312955274 s and Kernel 2 execeution time: 0.0013630261606886053 s.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krnl_1_obj = krnl_1_prog.mat_mul_1\n",
    "krnl_2_obj = krnl_2_prog.mat_mul_2\n",
    "\n",
    "start = end = None\n",
    "krnl_1_exec_time = krnl_2_exec_time = 0\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    sz, first_device, second_device, result = memory_stuff()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    krnl_1_obj(queue, (sz, sz), (8, 8), result, first_device, second_device)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    krnl_1_exec_time += (end-start)\n",
    "    \n",
    "for i in range(50):\n",
    "    \n",
    "    sz, first_device, second_device, result = memory_stuff()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    krnl_2_obj(queue, (sz, sz), (8, 8), result, first_device, second_device)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    krnl_2_exec_time += (end-start)\n",
    "    \n",
    "krnl_1_exec_time /= 100\n",
    "krnl_2_exec_time /= 100\n",
    "\n",
    "f\"Kernel 1 execution time: {krnl_1_exec_time} s and Kernel 2 execeution time: {krnl_2_exec_time} s.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the whole notebook multiple times and you'll see that they will essentially always be \"roughly the same speed.\" <br>\n",
    "This is because the buffers are in cache, and access from cache is going to be fast too -- evidently as fast as local memory access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are roughly the same speed. Kernel 2 is a bit faster.\n"
     ]
    }
   ],
   "source": [
    "if krnl_1_exec_time < krnl_2_exec_time:\n",
    "    \n",
    "    speedup = round(krnl_2_exec_time / krnl_1_exec_time)\n",
    "    \n",
    "    if speedup == 1:\n",
    "        \n",
    "        print(f\"They are roughly the same speed. Kernel 1 is a bit faster.\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Kernel 1 is roughly {speedup}x as fast as Kernel 2.\")\n",
    "else:\n",
    "    \n",
    "    speedup = round(krnl_1_exec_time / krnl_2_exec_time)\n",
    "    \n",
    "    if speedup == 1:\n",
    "        \n",
    "        print(f\"They are roughly the same speed. Kernel 2 is a bit faster.\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f\"Kernel 2 is roughly {speedup}x fast as Kernel 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
